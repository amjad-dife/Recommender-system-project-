{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Deployment "
      ],
      "metadata": {
        "id": "oTZJOwhmxlDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install library"
      ],
      "metadata": {
        "id": "KyGia_jgx0y0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2vULXIeD3UH",
        "outputId": "db4becac-c356-4910-c4b3-274499e341f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (0.79.0)\n",
            "Requirement already satisfied: starlette==0.19.1 in /usr/local/lib/python3.7/dist-packages (from fastapi) (0.19.1)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from fastapi) (1.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (4.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (3.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (1.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydentic in /usr/local/lib/python3.7/dist-packages (0.0.1.dev3)\n",
            "Requirement already satisfied: python-stdnum>=1.16 in /usr/local/lib/python3.7/dist-packages (from pydentic) (1.17)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydentic) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unicorn in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (0.79.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (1.5.5)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.1.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.7/dist-packages (0.18.2)\n",
            "Requirement already satisfied: starlette==0.19.1 in /usr/local/lib/python3.7/dist-packages (from fastapi) (0.19.1)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from fastapi) (1.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (3.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (4.1.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (1.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (2.10)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn) (0.13.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi\n",
        "!pip install pydentic\n",
        "!pip install pyngrok\n",
        "!pip install unicorn\n",
        "!pip install fastapi nest-asyncio pyngrok uvicorn\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import library"
      ],
      "metadata": {
        "id": "0gVbA_qGx-8l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T6DvzfAmEZqN"
      },
      "outputs": [],
      "source": [
        "from flask import Flask\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from inspect import Parameter\n",
        "from itertools import count\n",
        "from typing import Any, Dict\n",
        "from urllib.request import Request\n",
        "from fastapi import Body,FastAPI,Request\n",
        "from matplotlib.pyplot import text\n",
        "from pydantic import BaseModel, Field as PydanticField\n",
        "from pydantic.fields import Field\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Data_set/recommender_system"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZWXQ-_F4m7M",
        "outputId": "11500189-11cc-45e1-97d7-330f72c112fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data_set/recommender_system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbPN2EDMJFxk",
        "outputId": "02a57ece-1539-420a-d970-679213142ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "! ngrok authtoken \"2CgE5efiOHm9YtS0PAOAIiyZKmO_6s95xgufUcucd1Mu2ZTPx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "l5dhvykYEG81"
      },
      "outputs": [],
      "source": [
        "class model:\n",
        "    def __init__(self):\n",
        "        self.tv = TfidfVectorizer()\n",
        "        self.loaded_model = LogisticRegression()\n",
        "        self.loaded_model = pickle.load(open('finalized_model.sav', 'rb'))\n",
        "        self.tv = pickle.load(open('tfidf.pickle', 'rb'))\n",
        "        X = pickle.load(open('train_comment_features.pickle', 'rb'))\n",
        "        self.tv.fit(X)\n",
        "        self.df=pd.read_csv('book.csv')\n",
        "        self.book_matrix = self.df.pivot_table(index='userID', columns='title', values='rating')\n",
        "\n",
        "\n",
        "    def getCo(self,bookName):\n",
        "        # Collect the rate Similarities:\n",
        "        book_rate = self.book_matrix[bookName]\n",
        "        similar_to_air_book = self.book_matrix.corrwith(book_rate)\n",
        "        return similar_to_air_book\n",
        "\n",
        "    def recommend(self,bookName,feedBack):\n",
        "      if feedBack ==\"Positive\":\n",
        "        return self.getCo(bookName).idxmax()\n",
        "      else :\n",
        "        return self.getCo(bookName).idxmin()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def sa(self,text):\n",
        "        print(text)\n",
        "        t=self.tv.transform([text])\n",
        "        return self.loaded_model.predict(t)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gl-Iv1W8Kw21"
      },
      "outputs": [],
      "source": [
        "\n",
        "app =FastAPI()\n",
        "\n",
        "ml = model()\n",
        "df=pd.read_csv('book.csv')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "# Let's chat for 5 lines\n",
        "\n",
        "\n",
        "books=[]\n",
        "recommed_book=[]\n",
        "\n",
        "\n",
        "\n",
        "@app.post('/')\n",
        "async def home(info: Request):\n",
        "    req_info = await info.json()\n",
        "    print()\n",
        "    \n",
        "    intent_NAme = req_info[\"queryResult\"][\"intent\"][\"displayName\"]\n",
        "    text = \"\"\n",
        "    if(intent_NAme==\"TEST\"):\n",
        "       text =\"this is test intent\"\n",
        "    elif (intent_NAme==\"MovieName\"):\n",
        "       text =\"Move name is avatar\"\n",
        "\n",
        "\n",
        "    return {\"fulfillmentMessages\": [\n",
        "      {\n",
        "        \"text\": {\n",
        "          \"text\": [\n",
        "            text\n",
        "          ]\n",
        "        }\n",
        "      }\n",
        "    ]}\n",
        "\n",
        "@app.post('/SA')\n",
        "async def SA(info: Request):\n",
        "    req_info = await info.json()\n",
        "    print()\n",
        "    intent_NAme = req_info[\"queryResult\"][\"queryText\"]\n",
        "    text = str(ml.sa(intent_NAme))\n",
        "    books.append([recommend,text])\n",
        "    return {\"fulfillmentMessages\": [\n",
        "      {\n",
        "        \"text\": {\n",
        "          \"text\": [\n",
        "            text\n",
        "          ]\n",
        "        }\n",
        "      }\n",
        "    ]}\n",
        "@app.post('/Diolgpt')\n",
        "async def ranam(info: Request):\n",
        "\n",
        "    with open('HISTORY.txt','r') as f:\n",
        "        History_id=f.readlines()\n",
        "        History_id=int(History_id)\n",
        "        print(History_id)\n",
        "    req_info = await info.json()\n",
        "    print()\n",
        "    intent_NAme = req_info[\"queryResult\"][\"queryText\"]\n",
        "    text = df.sample()['title'].values[0]\n",
        "\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(intent_NAme + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([History_id, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "    text=tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    with open('HISTORY.txt','w') as f:\n",
        "      History_id+=1\n",
        "      f.write(History_id)\n",
        "    return {\"fulfillmentMessages\": [\n",
        "      {\n",
        "        \"text\": {\n",
        "          \"text\": [\n",
        "            text\n",
        "          ]\n",
        "        }\n",
        "      }\n",
        "    ]}\n",
        "@app.post('/recommend')\n",
        "async def recommend(info: Request):\n",
        "\n",
        "    req_info = await info.json()\n",
        "    intent_NAme = req_info[\"queryResult\"][\"intent\"][\"displayName\"]\n",
        "    intent_NAme2 = req_info[\"queryResult\"][\"queryText\"]\n",
        "    text = str(ml.sa(intent_NAme))\n",
        "    if(intent_NAme==\"recommender followup\"):\n",
        "      with open('temp.txt','r') as f:\n",
        "        recommed_book=f.readlines()\n",
        "        print(recommed_book)\n",
        "      text_sa = str(ml.sa(intent_NAme2))[1:-1]\n",
        "      print(text_sa)\n",
        "      text =ml.recommend(recommed_book[0],text_sa)\n",
        "      recommed_book=text\n",
        "    elif (intent_NAme==\"recommender\"):\n",
        "      text = df.sample()['title'].values[0]\n",
        "      recommed_book =text\n",
        "    with open('temp.txt','w') as f:\n",
        "      f.write(recommed_book)\n",
        "    \n",
        "    \n",
        "    return {\"fulfillmentMessages\": [\n",
        "      {\n",
        "        \"text\": {\n",
        "          \"text\": [\n",
        "            text\n",
        "          ]\n",
        "        }\n",
        "      }\n",
        "    ]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9nTBCyXJ3zK",
        "outputId": "fc48019e-bc15-4132-e1cc-b85e7bbdfbfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://5323-35-204-49-246.ngrok.io\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1464]\n",
            "INFO:uvicorn.error:Started server process [1464]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:uvicorn.error:Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:uvicorn.error:Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "INFO:uvicorn.error:Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ],
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "# Let's chat for 5 lines\n",
        "for step in range(5):\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "zXzdgwC4M5bC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Group8_deployment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}